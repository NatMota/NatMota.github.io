{"componentChunkName":"component---src-templates-blog-template-js","path":"/blogs/trying-out-compact-llms-for-product-work/","result":{"data":{"blog":{"content":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"heading-4\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Motivation\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"I wanted to recreate a personal assistant environment for product managers who:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Do not have access to, or are not allowed to place sensitive data on online services\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Want to use software for free\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Know python but have no time to learn related tools \",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"May be concerned with software licensing restrictions\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Are curious about LLM's and are exposed to them already\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Do not have a powerful machine to run LLM's\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"heading-4\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Mise en place\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"I'm using an apple silicon mac. Use ChatGPT or Claude to guide you through system specific setup.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Computer with about 1.5GB of disk space, about 2GB of free ram, CUDA gpu a plus\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Used a Mac air M1 fine. \",\"marks\":[],\"data\":{}}]}]}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Python IDE of your choice\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Create a new python environment:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"python -m venv llm_env\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"source llm_env/bin/activate \",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"python -m pip install --upgrade pip\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"pip install transformers torch\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]}]}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Essential facts and lexicon\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Large language models (LLM's) are systems specialised in handling \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"language\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" by\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"1. Being painstakingly baked to generate 'correct' \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"language\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" patterns\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"2. Taking a \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"language\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" input and converting it into mathematical symbols\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"3. Select the next best mathematical symbols based on previous conditioning and convert the result into \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"language \",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"output\",\"marks\":[],\"data\":{}}]}]}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"ChatGPT is an LLM that has an additional layer of frosting to specifically work better in two way conversations\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Language input into an LLM is a prompt\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The prompt becomes the seed not just for the answer, but for the answering itself. It can have a dramatic effect on the quality of the response, and has led to the rise of the field of '\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://www.promptingguide.ai/introduction/tips\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"prompt engineering\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"'\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://www.promptingguide.ai/techniques/zeroshot\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Zero-shot\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" prompting directly instructs the model to perform a task without any additional examples to steer it\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"LLM's are baked to answer a question 'correctly' by matching the core concepts of the prompt to trained language structures\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This means that much like a conversation, the output will never be exactly the same across two equal prompts\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Software code is much more structured than human language, so LLM's will introduce less variety when looking for the right solution, but variety is still possible\",\"marks\":[],\"data\":{}}]}]}]}]}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Prompts are converted into tokens. As a simple example, every word in a prompt will be matched to a token. Imagine tokens as numbers that correspond to a specific word. \",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"LLM's can be made more effective by:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"improving the amount and quality of language data available to condition them \",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"improving their ability to detect language structures and hierarchy, their \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"depth\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"improving their ability to process more information in each conversation or prompt, their \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"width\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]}]}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"One simple way to compare how much information LLM's can meaningfully process in a conversation or a prompt response is to compare their context window\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"A context window is the limit in the chain of tokens that an LLM can hold at once when replying in a conversation or prompt, including the conversation history \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"AND response\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"For example, Chat GPT's free version has a context window of 8k tokens, but the enterprise version is 128k tokens -  This means it can operate between: \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"12-13 pages of single-spaced text in the free version if you imagined it as one doc\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"197 pages doc on the enterprise version\",\"marks\":[],\"data\":{}}]}]}]}]}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Commercial conversational LLM's are extensively refined and factually enriched i.e. with true historical events and mathematical reasoning... which means a lot of manual work goes into it and gives you an inflated impression of their raw ability! \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"For this alone it is very educational to play with non-tuned LLM's\",\"marks\":[],\"data\":{}}]}]}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"An introduction I like from S Wolfram on the subject of LLMs is available \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"here\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]}]}]}]},{\"nodeType\":\"heading-4\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Seasonal sauce making\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This technology is moving very fast, so make sure your pick fresh LLM's for your work. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"I have sourced LLM's from Hugging Face. This is the go-to place to learn about new models and compare them. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"We want to be able to run this in any machine where you're allowed to run python no matter the corporate environment. I know what is like to be limited to just using VBA to survive so if you're not allowed to use python then do not try this at work. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"That said, this approach just needs python, and the models I used here are open. I have only used them for personal reasons and you may still need to verify the license requirements for your specific use case. To the best of my ability, no data is then sent online with this approach. So hopefully it's a quick and safe solution to get you going. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"I used Hugging Face's open LLM scoreboard \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"here\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"I picked HF's own \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://huggingface.co/HuggingFaceTB/SmolLM-360M\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Smol 360M model\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". The number means it has 360 Million adjustments to the neurons of the model, assembled in the structure described \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://huggingface.co/blog/smollm\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"here\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This model is free to use commercially\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This model is fairly small\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This model is quite fresh, less than a month old at writing\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Other factors that make this 'open' are the tools needed to reconstruct it\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"training data\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" is available online\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The model weights are available to be downloaded - this is crucial to being able to modify the model from different save points at which it was created\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Closed models like ChatGPT do not make their weights available\",\"marks\":[],\"data\":{}}]}]}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Other supporting information needed to create output is available \",\"marks\":[],\"data\":{}}]}]}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Online demo available \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://huggingface.co/spaces/HuggingFaceTB/SmolLM-360M-Instruct-WebGPU\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"here\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Hugging face is great to help you explore models, with it's account you can get licensing for other more closed and more advanced models, and it's the go-to place for the community to release new models and compare their effectiveness using standard tests. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This made me realise the heft of LLM's. A rough rule of thumb is that you need 2X GB of disk and RAM to run a model with X billion parameters. Even more to bake a new one. So I might be able to run Llama 7b on my gaming rig, but no chance with the 405b version... So to get a GPT4 competitor in a machine you'd need 900GB of space, 1 TB of RAM, and strong video card capabilities. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Note how there are two versions of Smol 360M\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://huggingface.co/HuggingFaceTB/SmolLM-360M\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"360M LLM\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://huggingface.co/HuggingFaceTB/SmolLM-360M-Instruct\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"360M LLM - instruct version\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This is 360M model with a conversational layer\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In the same way, the ChatGPT model can be ported to GPT2, GPT3, and could even be ported to a nonGPT LLM, possibly\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"I implemented it with a chat runtime using doe suggested by Claude and it had pretty disappointing results, so I stopped using it. Closed source conversation layers are a big market advantage at the moment!\",\"marks\":[],\"data\":{}}]}]}]}]}]},{\"nodeType\":\"heading-4\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Roasting (my CPU)\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Get a code sample from ChatGPT or Claude. This worked on my machine: \\n\\n\",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"# pip install transformers\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"# pip install torch==2.2.0 - compatible with M1\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"from transformers import AutoModelForCausalLM, AutoTokenizer\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"checkpoint = \\\"HuggingFaceTB/SmolLM-360M\\\"\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"device = \\\"cpu\\\" # for GPU usage or \\\"cpu\\\" for CPU usage\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"tokenizer = AutoTokenizer.from_pretrained(checkpoint)\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\\\"auto\\\")`\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"test_intro_prompt_3 = f\\\"\\\"\\\"Write an article about the benefits of meditation.\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\nIntroduction:[Write an introduction about meditation.]\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\nBenefits:\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"1. [Benefit 1]\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"2. [Benefit 2]\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"3. [Benefit 3]\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\nConclusion:[Write a conclusion about the benefits of meditation.]\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"inputs = tokenizer.encode(test_intro_prompt_3, return_tensors=\\\"pt\\\", truncation=True, max_length=2000).to(device)\\n\\noutputs = model.generate(        inputs,        max_new_tokens=300,        temperature=0.2,        top_p=0.95,        do_sample=True    )print(tokenizer.decode(outputs[0]))\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Now save this as a .py file and run it on your terminal:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"(llm_env) (base) ... personal LLM % python smol.py    \",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"And then wait. If you do this the first time, it will download the model on your machine. There you go! Your first personal LLM is alive. For example if you wanted to try something like Google Gemma, this is where you'd need to authenticate with your Hugging Face account, having already accepted their terms of use. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]}","references":[]},"title":"Trying out compact LLMs for product work","image":{"gatsbyImageData":{"images":{"sources":[{"srcSet":"https://images.ctfassets.net/bgaxkx6xut76/oAWyY6WnETUF66EGlynpk/18f60666fc726b55335e3729ffd7b74d/Screenshot_2024-08-11_at_21.13.30.png?w=317&h=77&q=50&fm=webp 317w,\nhttps://images.ctfassets.net/bgaxkx6xut76/oAWyY6WnETUF66EGlynpk/18f60666fc726b55335e3729ffd7b74d/Screenshot_2024-08-11_at_21.13.30.png?w=634&h=153&q=50&fm=webp 634w,\nhttps://images.ctfassets.net/bgaxkx6xut76/oAWyY6WnETUF66EGlynpk/18f60666fc726b55335e3729ffd7b74d/Screenshot_2024-08-11_at_21.13.30.png?w=1268&h=306&q=50&fm=webp 1268w","sizes":"(min-width: 1268px) 1268px, 100vw","type":"image/webp"}],"fallback":{"src":"https://images.ctfassets.net/bgaxkx6xut76/oAWyY6WnETUF66EGlynpk/18f60666fc726b55335e3729ffd7b74d/Screenshot_2024-08-11_at_21.13.30.png?w=1268&h=306&q=50&fm=png","srcSet":"https://images.ctfassets.net/bgaxkx6xut76/oAWyY6WnETUF66EGlynpk/18f60666fc726b55335e3729ffd7b74d/Screenshot_2024-08-11_at_21.13.30.png?w=317&h=77&q=50&fm=png 317w,\nhttps://images.ctfassets.net/bgaxkx6xut76/oAWyY6WnETUF66EGlynpk/18f60666fc726b55335e3729ffd7b74d/Screenshot_2024-08-11_at_21.13.30.png?w=634&h=153&q=50&fm=png 634w,\nhttps://images.ctfassets.net/bgaxkx6xut76/oAWyY6WnETUF66EGlynpk/18f60666fc726b55335e3729ffd7b74d/Screenshot_2024-08-11_at_21.13.30.png?w=1268&h=306&q=50&fm=png 1268w","sizes":"(min-width: 1268px) 1268px, 100vw"}},"layout":"constrained","backgroundColor":"#181818","width":1268,"height":306}}},"contentfulBlogPostDescriptionTextNode":{"description":"I tried to find a safe way to run an LLM on your personal computer without licensing or data transfer worries. "}},"pageContext":{"slug":"trying-out-compact-llms-for-product-work"}},"staticQueryHashes":["3303551625"],"slicesMap":{}}